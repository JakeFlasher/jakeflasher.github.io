* Mdnotes File Name: [[eeckhoutComputerArchitecturePerformance2010a]]

# Annotations(2021-11-08)
“Recent work also focused on generating synthetic benchmarks rather than synthetic traces. Hsieh and Pedram [86] generate a fully functional program from a statistical profile. However, all the characteristics in the statistical profile are microarchitecture-dependent, which makes this technique useless for microprocessor design space explorations. Bell and John [13] generate short synthetic benchmarks using a collection of microarchitecture-independent and microarchitecturedependent characteristics similar to what is done in statistical simulation as described in this chapter. Their goal is performance model validation of high-level architectural simulators against RTL-level cycle-accurate simulators using small but representative synthetic benchmarks. Joshi et al. [100] generate synthetic benchmarks based on microarchitecture-independent characteristics only, and they leverage that framework to automatically generate stressmarks (i.e., synthetic programs that maximize power consumption, temperature, etc.), see [101].” (Eeckhout, 2010, p. 93)
“A couple other research efforts model cache performance in a statistical way. Berg and Hagersten [14] propose light-weight profiling to collect a memory reference reuse distribution at low overhead and then estimate cache miss rates for random-replacement caches. Chandra et al. [24] propose performance models to predict the impact of cache sharing on co-scheduled programs. The output provided by the performance model is an estimate of the number of extra cache misses for each thread due to cache sharing.” (Eeckhout, 2010, p. 93)
“A nagging issue to architectural simulation relates to validation. Performance simulators model a computer architecture at some level of detail, and they typically do not model the target architecture in a cycle-accurate manner; this is especially true for academic simulators. (Although these simulators are often referred to as cycle-accurate simulators, a more appropriate term would probably be cyclelevel simulators.) The reason for the lack of validation is threefold. For one, although the high-level design decisions are known, many of the details of contemporary processors are not well described so that academics can re-implement them in their simulators. Second, implementing those details is time-consuming and is likely to severely slowdown the simulation. Third, research ideas (mostly) target future processors for which many of the details are not known anyway, so a generic processor model is a viable option. (This is even true in industry during the early stages of the design cycle.) Desikan et al. [40] describe the tedious validation process of the sim-alpha simulator against the Alpha 21264 processor. They were able to get the mean error between the hardware and the simulator down to less than 2% for a set of microbenchmarks; however, the errors were substantially higher for the SPEC CPU2000 benchmarks (20% on average).” (Eeckhout, 2010, p. 105)


